{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of HACKANONSs COLAB 25GB RAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajinchal/Generating-sonnet/blob/master/SONNET_%20GENERATOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7IOR_6yffU9",
        "colab_type": "text"
      },
      "source": [
        "# **IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjrBqkQPD_xq",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import Regularizer\n",
        "import tensorflow.keras.utils as ku \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import re"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuCBB-YzfrpG",
        "colab_type": "text"
      },
      "source": [
        "# **READING, TOKENIZING AND CLEANING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "#Reading Data\n",
        "data = open(r'/content/drive/My Drive/dataset.txt').read()\n",
        "\n",
        "#Making sentences Lower case and spliting it \n",
        "sonnet_14 = data.lower().split(\"<eos>\")\n",
        "\n",
        "#Removing the un wanted terms from the data\n",
        "for i in range(len(sonnet_14)):\n",
        " sonnet_14[i] = sonnet_14[i].replace(\"\\n\",'')\n",
        " sonnet_14[i] = sonnet_14[i].replace(\"<eos>\",'')\n",
        " sonnet_14[i] = re.sub(\"[`~!@#$+%*:()'?-]\", ' ',sonnet_14[i]) \n",
        " sonnet_14[i] = remove_stopwords(sonnet_14[i])\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTAqPjEwf88r",
        "colab_type": "text"
      },
      "source": [
        "# **GENERATE VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlUrZhRiEJPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generating the values for the data\n",
        "tokenizer.fit_on_texts(sonnet_14)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "input_sequences = []\n",
        "for line in sonnet_14:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_sequence)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUhBviehEOUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sequence = max([len(x) for x in input_sequences])#selecting the maximum input sequence length\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence, padding='pre'))#padding prior zeros to the input sequence\n",
        "\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]#splitting the data into predictor and label\n",
        "                                                                                  \n",
        "label = ku.to_categorical(label, num_classes=total_words)#making the variables to be either 0 or 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hmLmG4LgJ5Q",
        "colab_type": "text"
      },
      "source": [
        "# **CREATING A MODEL AND ITS LAYERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY9FX92qETno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential() #Using sequential model\n",
        "model.add(Embedding(total_words, 50, input_length=max_sequence-1))  # Your Embedding Layer\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))  # An LSTM Layer\n",
        "model.add(Dropout(0.2))  #(# A dropout layer)\n",
        "model.add(LSTM(100))  #(# Another LSTM Layer)\n",
        "model.add(Dense(total_words/2, activation='relu'))  # A Dense Layer including regularizers\n",
        "model.add(Dense(total_words, activation='softmax'))  # A Dense Layer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Pick a loss function and an optimizer)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk8pa26MgXAE",
        "colab_type": "text"
      },
      "source": [
        "# **TRAINING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIa4-HI0xjle",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6dfb7da-3fbf-4c3c-bb34-5619ead499ab"
      },
      "source": [
        "train=model.fit(predictors,label,epochs=100,verbose=1)#traing the model on 100 epochs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8371/8371 [==============================] - 307s 37ms/step - loss: 7.1205 - accuracy: 0.0663\n",
            "Epoch 2/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 6.6212 - accuracy: 0.0824\n",
            "Epoch 3/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 6.2654 - accuracy: 0.0928\n",
            "Epoch 4/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 5.9854 - accuracy: 0.1012\n",
            "Epoch 5/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 5.7454 - accuracy: 0.1100\n",
            "Epoch 6/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 5.5269 - accuracy: 0.1182\n",
            "Epoch 7/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 5.3247 - accuracy: 0.1275\n",
            "Epoch 8/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 5.1280 - accuracy: 0.1374\n",
            "Epoch 9/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 4.9341 - accuracy: 0.1499\n",
            "Epoch 10/100\n",
            "8371/8371 [==============================] - 306s 36ms/step - loss: 4.7360 - accuracy: 0.1647\n",
            "Epoch 11/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 4.5243 - accuracy: 0.1820\n",
            "Epoch 12/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 4.3102 - accuracy: 0.2031\n",
            "Epoch 13/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 4.0884 - accuracy: 0.2260\n",
            "Epoch 14/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 3.8692 - accuracy: 0.2531\n",
            "Epoch 15/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 3.6648 - accuracy: 0.2798\n",
            "Epoch 16/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 3.4790 - accuracy: 0.3062\n",
            "Epoch 17/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 3.3138 - accuracy: 0.3301\n",
            "Epoch 18/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 3.1596 - accuracy: 0.3540\n",
            "Epoch 19/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 3.0245 - accuracy: 0.3756\n",
            "Epoch 20/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.8987 - accuracy: 0.3954\n",
            "Epoch 21/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.7814 - accuracy: 0.4160\n",
            "Epoch 22/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.6816 - accuracy: 0.4320\n",
            "Epoch 23/100\n",
            "8371/8371 [==============================] - 301s 36ms/step - loss: 2.5889 - accuracy: 0.4485\n",
            "Epoch 24/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.4967 - accuracy: 0.4641\n",
            "Epoch 25/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.4175 - accuracy: 0.4780\n",
            "Epoch 26/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 2.3459 - accuracy: 0.4914\n",
            "Epoch 27/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 2.2803 - accuracy: 0.5038\n",
            "Epoch 28/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 2.2188 - accuracy: 0.5147\n",
            "Epoch 29/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 2.1617 - accuracy: 0.5256\n",
            "Epoch 30/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 2.1092 - accuracy: 0.5357\n",
            "Epoch 31/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 2.0596 - accuracy: 0.5452\n",
            "Epoch 32/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 2.0154 - accuracy: 0.5536\n",
            "Epoch 33/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.9754 - accuracy: 0.5622\n",
            "Epoch 34/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.9346 - accuracy: 0.5707\n",
            "Epoch 35/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.9009 - accuracy: 0.5756\n",
            "Epoch 36/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.8650 - accuracy: 0.5845\n",
            "Epoch 37/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.8302 - accuracy: 0.5904\n",
            "Epoch 38/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.8004 - accuracy: 0.5969\n",
            "Epoch 39/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.7732 - accuracy: 0.6037\n",
            "Epoch 40/100\n",
            "8371/8371 [==============================] - 307s 37ms/step - loss: 1.7479 - accuracy: 0.6083\n",
            "Epoch 41/100\n",
            "8371/8371 [==============================] - 307s 37ms/step - loss: 1.7240 - accuracy: 0.6130\n",
            "Epoch 42/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.7029 - accuracy: 0.6173\n",
            "Epoch 43/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.6746 - accuracy: 0.6238\n",
            "Epoch 44/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.6555 - accuracy: 0.6277\n",
            "Epoch 45/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.6366 - accuracy: 0.6315\n",
            "Epoch 46/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.6164 - accuracy: 0.6366\n",
            "Epoch 47/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.5997 - accuracy: 0.6395\n",
            "Epoch 48/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.5831 - accuracy: 0.6434\n",
            "Epoch 49/100\n",
            "8371/8371 [==============================] - 306s 36ms/step - loss: 1.5658 - accuracy: 0.6469\n",
            "Epoch 50/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.5522 - accuracy: 0.6500\n",
            "Epoch 51/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.5333 - accuracy: 0.6540\n",
            "Epoch 52/100\n",
            "8371/8371 [==============================] - 307s 37ms/step - loss: 1.5208 - accuracy: 0.6569\n",
            "Epoch 53/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.5106 - accuracy: 0.6594\n",
            "Epoch 54/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4950 - accuracy: 0.6630\n",
            "Epoch 55/100\n",
            "8371/8371 [==============================] - 308s 37ms/step - loss: 1.4859 - accuracy: 0.6649\n",
            "Epoch 56/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4691 - accuracy: 0.6675\n",
            "Epoch 57/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.4607 - accuracy: 0.6710\n",
            "Epoch 58/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4479 - accuracy: 0.6735\n",
            "Epoch 59/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4383 - accuracy: 0.6757\n",
            "Epoch 60/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.4297 - accuracy: 0.6779\n",
            "Epoch 61/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.4206 - accuracy: 0.6800\n",
            "Epoch 62/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.4095 - accuracy: 0.6823\n",
            "Epoch 63/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4052 - accuracy: 0.6832\n",
            "Epoch 64/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3933 - accuracy: 0.6857\n",
            "Epoch 65/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3863 - accuracy: 0.6877\n",
            "Epoch 66/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3759 - accuracy: 0.6903\n",
            "Epoch 67/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3721 - accuracy: 0.6907\n",
            "Epoch 68/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.3622 - accuracy: 0.6937\n",
            "Epoch 69/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3563 - accuracy: 0.6957\n",
            "Epoch 70/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3495 - accuracy: 0.6966\n",
            "Epoch 71/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3427 - accuracy: 0.6978\n",
            "Epoch 72/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3364 - accuracy: 0.7002\n",
            "Epoch 73/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3273 - accuracy: 0.7014\n",
            "Epoch 74/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3218 - accuracy: 0.7030\n",
            "Epoch 75/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3179 - accuracy: 0.7037\n",
            "Epoch 76/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3070 - accuracy: 0.7071\n",
            "Epoch 77/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3061 - accuracy: 0.7067\n",
            "Epoch 78/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3004 - accuracy: 0.7079\n",
            "Epoch 79/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.2958 - accuracy: 0.7088\n",
            "Epoch 80/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2888 - accuracy: 0.7110\n",
            "Epoch 81/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2838 - accuracy: 0.7121\n",
            "Epoch 82/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2808 - accuracy: 0.7130\n",
            "Epoch 83/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2792 - accuracy: 0.7135\n",
            "Epoch 84/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2739 - accuracy: 0.7149\n",
            "Epoch 85/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2645 - accuracy: 0.7175\n",
            "Epoch 86/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2645 - accuracy: 0.7171\n",
            "Epoch 87/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2578 - accuracy: 0.7185\n",
            "Epoch 88/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2558 - accuracy: 0.7196\n",
            "Epoch 89/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2489 - accuracy: 0.7212\n",
            "Epoch 90/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2473 - accuracy: 0.7217\n",
            "Epoch 91/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2408 - accuracy: 0.7233\n",
            "Epoch 92/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2365 - accuracy: 0.7241\n",
            "Epoch 93/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2329 - accuracy: 0.7244\n",
            "Epoch 94/100\n",
            "8371/8371 [==============================] - 306s 36ms/step - loss: 1.2295 - accuracy: 0.7256\n",
            "Epoch 95/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.2286 - accuracy: 0.7257\n",
            "Epoch 96/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.2253 - accuracy: 0.7273\n",
            "Epoch 97/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.2226 - accuracy: 0.7273\n",
            "Epoch 98/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2192 - accuracy: 0.7288\n",
            "Epoch 99/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2174 - accuracy: 0.7289\n",
            "Epoch 100/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2141 - accuracy: 0.7305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pvb64LSgqpI",
        "colab_type": "text"
      },
      "source": [
        "# **SAVING THE TRAINED MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpoO9NJIy3Qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c7407248-85e0-483c-bffd-f1eadc895261"
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"jack1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rASNH2Fzg-Lk",
        "colab_type": "text"
      },
      "source": [
        "# **LOADING TRAINED MODEL AND PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qbmrSROwEE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e231015e-565e-4e50-84f3-7164cc3ebcc6"
      },
      "source": [
        "#to read the .json and HDF5 file from the drive\n",
        "from tensorflow.keras.models import model_from_json\n",
        "json_file = open('/content/drive/My Drive/jack1.json','r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights('/content/drive/My Drive/model.h5')\n",
        "model = loaded_model\n",
        "#Feeding the trained model to predict the words\n",
        "def predict(seed_text , seed = 10):\n",
        "    for i in range( seed ):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=\n",
        "        max_sequence , padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0 )\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "\n",
        "#Giving seed text for generation of the sonnet\n",
        "data = open(r'/content/drive/My Drive/dataset2.txt').read()\n",
        "data = data.split()\n",
        "sonnet_14 = []\n",
        "\n",
        "for i in range(len(data)):  \n",
        "  data[i] = re.sub(\"[,`~!@#$+%*:()'?-]\", '',data[i]) \n",
        "  data[i] = remove_stopwords(data[i])\n",
        "for i in range(len(data)):\n",
        "  if(data[i]!='<eos>'and data[i]!='.'and data[i]!=''):\n",
        "    sonnet_14.append(data[i])\n",
        "\n",
        "sonnet_14=sonnet_14[:70]\n",
        "count=0\n",
        "for i in sonnet_14:\n",
        "  count=count+1\n",
        "  if count>1 and count%15==0:\n",
        "    print('\\n')\n",
        "    count+=1\n",
        "  print(predict(i))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "let day s patient long heaven s gates white things slave\n",
            "forget days flag heart gaze d fields frames fair wood fair\n",
            "matters rest sacred men heath bewayles breast d length d laid\n",
            "reigned pass thou shall soule thou straight love desire assay golden\n",
            "oer d sad breath tell d arms youth rise thou wit\n",
            "happy lift like gives d miser s disguise happy ofttimes gods\n",
            "realms s number mightie design eastward round time worth thought vain\n",
            "long s smoke ghost thee pure s safer undo doth child\n",
            "ago lay n right o s years chubby woman sound heavy\n",
            "talked eye s despair — largely ’ forms blind hadde hart\n",
            "love s spacious ploughman toil s wedded spotless olympian guide o\n",
            "let day s patient long heaven s gates white things slave\n",
            "voices car d s caressed computation honour friends grace n d\n",
            "low thy s tender behold forward friends hate ah hate d\n",
            "\n",
            "\n",
            "ruled d got love die ye king art hope shone child\n",
            "brief breathe thou st heel away night feast s passing tell\n",
            "sessions spectral thy look succour wall love famine thy reach d\n",
            "royally bears thy st voice o darkness d think thy è\n",
            "sung present man weak dost stood crawls fade single king green\n",
            "laughed place sounds great new thee lights ne praise shed soul\n",
            "wept happiness — complain lie ev dost è lady doth mother\n",
            "maybe “ new earth nature day place like thought thee pleasure\n",
            "availed priestess s notices boiling thy shall shadow crown blind doth\n",
            "let day s patient long heaven s gates white things slave\n",
            "better th s thro death ayre thou mind friend twilight hand\n",
            "know warre men glow chase yonder caressed ll d fear thy\n",
            "poor hand satin look prayers hath warm climb poor hand lead\n",
            "thing heard friend self livery d helen answer silent brings soul\n",
            "\n",
            "\n",
            "lost felt lost came shall native — gaze hand grace o\n",
            "yesterday beauty long fly chains ve rest air thought grieve s\n",
            "kissed er good ‘ goodliest sweet smoldering fair time wild thou\n",
            "lips blessed laurels days er sees towers st waves nature fame\n",
            "thrill love s break perfect let morn — holy s morn\n",
            "shake thou wonders heart land theirs star day ashes love tread\n",
            "dew d purple came let argument tell heavens thither shalt grace\n",
            "drenched sorrow st divine divine graceful praise spring time rise hate\n",
            "lids thy delights weapon shall mount height hand grace evil live\n",
            "— thee days morrow old heart love greatnes lids born doth\n",
            "missed black men glow troubled round flowers shall red thou star\n",
            "regret — dost happy sad world long st ear thy drop\n",
            "kiss death despised evening better th thee death time worth broom\n",
            "shot meek coming thy s naked beguiled days shone love tree\n",
            "\n",
            "\n",
            "sharp d gun scarcely rage away soul tell d seen loud\n",
            "breaths priestess s notices boiling thy shall shadow crown blind doth\n",
            "failing d certain midnight s possessed thou s repose beheld maying\n",
            "day world man powers look school th death king st false\n",
            "worn o heart snow away praise beauty eyes set old life\n",
            "eyes s rome lay people shall sleepless muses heart seek hand\n",
            "wet s lie god end man ‘ time height place day\n",
            "waste d hall gold god hath ’ bring thy sudden thou\n",
            "tears man ruin green wives let floods ruffled thought st crown\n",
            "let day s patient long heaven s gates white things slave\n",
            "forget days flag heart gaze d fields frames fair wood fair\n",
            "hills soul s reverend mighty thy billowy holds know calmly d\n",
            "gleaming thou air er light er wine visit trees length dear\n",
            "brass god heart pours dead hoarsely lifts book hate half è\n",
            "\n",
            "\n",
            "bronze space d hadst dead rest ages “ pass ’ wail\n",
            "peaks s live thee haste “ ll ” gaze o blue\n",
            "mesas priestess s notices boiling thy shall shadow crown blind doth\n",
            "brazen turret thespis o throw drama play sport hand common nature\n",
            "molten priestess s notices boiling thy shall shadow crown blind doth\n",
            "sea s act asleep thy oh sullen reason tyranny hand grace\n",
            "een spring ere d hero gesture gentler wrinkles o heart chance\n",
            "heaven rung arise love sturdy come round selfish stand haste born\n",
            "s best truth care friends far thee tears sad youth known\n",
            "blue groan d star dream storms breath walls youth thou prayers\n",
            "infinity bears thy st voice o darkness d think thy è\n",
            "undimmed little refreshes god anne world o shall shadows summers warm\n",
            "kindly “ morning brink sighs kindly god days death doubt thou\n",
            "cloud consoled cause hours accents d lamented seek praise valley pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqJn2xZtWa7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}