{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SONNET_GENERATOR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toukeer123/Adalovelace_day_Hackathon/blob/master/SONNET_GENERATOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7IOR_6yffU9"
      },
      "source": [
        "# **IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjrBqkQPD_xq"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import Regularizer\n",
        "import tensorflow.keras.utils as ku \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcIjR6RWXNr_",
        "outputId": "3a64aeef-2647-4eba-ad36-829967ee051a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuCBB-YzfrpG"
      },
      "source": [
        "# **READING, TOKENIZING AND CLEANING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "#Reading Data\n",
        "data = open(r'/content/drive/My Drive/dataset.txt').read()\n",
        "\n",
        "#Making sentences Lower case and spliting it \n",
        "sonnet_14 = data.lower().split(\"<eos>\")\n",
        "\n",
        "#Removing the un wanted terms from the data\n",
        "for i in range(len(sonnet_14)):\n",
        " sonnet_14[i] = sonnet_14[i].replace(\"\\n\",'')\n",
        " sonnet_14[i] = sonnet_14[i].replace(\"<eos>\",'')\n",
        " sonnet_14[i] = re.sub(\"[`~!@#$+%*:()'?-]\", ' ',sonnet_14[i]) \n",
        " sonnet_14[i] = remove_stopwords(sonnet_14[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTAqPjEwf88r"
      },
      "source": [
        "# **GENERATE VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlUrZhRiEJPT"
      },
      "source": [
        "#Generating the values for the data\n",
        "tokenizer.fit_on_texts(sonnet_14)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "input_sequences = []\n",
        "for line in sonnet_14:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUhBviehEOUi"
      },
      "source": [
        "max_sequence = max([len(x) for x in input_sequences])#selecting the maximum input sequence length\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence, padding='pre'))#padding prior zeros to the input sequence\n",
        "\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]#splitting the data into predictor and label\n",
        "                                                                                  \n",
        "label = ku.to_categorical(label, num_classes=total_words)#making the variables to be either 0 or 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hmLmG4LgJ5Q"
      },
      "source": [
        "# **CREATING A MODEL AND ITS LAYERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY9FX92qETno"
      },
      "source": [
        "model = Sequential() #Using sequential model\n",
        "model.add(Embedding(total_words, 50, input_length=max_sequence-1))  # Your Embedding Layer\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))  # An LSTM Layer\n",
        "model.add(Dropout(0.2))  #(# A dropout layer)\n",
        "model.add(LSTM(100))  #(# Another LSTM Layer)\n",
        "model.add(Dense(total_words/2, activation='relu'))  # A Dense Layer including regularizers\n",
        "model.add(Dense(total_words, activation='softmax'))  # A Dense Layer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Pick a loss function and an optimizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk8pa26MgXAE"
      },
      "source": [
        "# **TRAINING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIa4-HI0xjle"
      },
      "source": [
        "train=model.fit(predictors,label,epochs=100,verbose=1)#traing the model on 100 epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pvb64LSgqpI"
      },
      "source": [
        "# **SAVING THE TRAINED MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpoO9NJIy3Qd",
        "outputId": "c7407248-85e0-483c-bffd-f1eadc895261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"jack1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rASNH2Fzg-Lk"
      },
      "source": [
        "# **LOADING TRAINED MODEL AND PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qbmrSROwEE9",
        "outputId": "216763a7-09a8-424d-bc49-7ca935174119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "#to read the .json and HDF5 file from the drive\n",
        "from tensorflow.keras.models import model_from_json\n",
        "json_file = open('/content/drive/My Drive/jack1.json','r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights('/content/drive/My Drive/model.h5')\n",
        "model = loaded_model\n",
        "#Feeding the trained model to predict the words\n",
        "def predict(seed_text , seed = 625):\n",
        "    for i in range( seed ):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=\n",
        "        max_sequence , padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0 )\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "        if i!=1 and i!=0:\n",
        "          if i%13==0:\n",
        "            seed_text+='\\n'\n",
        "    return seed_text\n",
        "\n",
        "\n",
        "#Giving input and printing the sonnet\n",
        "sonnet_14 = input()\n",
        "tk=predict(sonnet_14)\n",
        "nxt=tk.split(\"\\n\")[1:15]\n",
        "for i in range(len(nxt)):  \n",
        "  nxt[i] = re.sub(\"[,`~!@#$+%*:()'?-]\", '',nxt[i])\n",
        "gt=nxt.pop(0).split(\" \")\n",
        "if(gt[0].lower()=='d' or gt[0].lower()=='s'or gt[0].lower()=='è'):gt[0]=\"As\"\n",
        "for i in([\" \".join(gt).capitalize()]+nxt):\n",
        "  print(i)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wa wa kya bat\n",
            " beauty clear doth outcry hearts hand grace o soft thy mortal d form\n",
            " thy power day power man grave thy sounds time summer eyes thee true\n",
            " blame death grave like blue d art fair known tis art power country\n",
            " doth è spirit brown happy save thou strong gentle come air thee address\n",
            " lies doth fair cast dear death true billowy thy mother ‘ songs throng\n",
            " s door case arms thee grief dim nature cast eyes heard “ blue\n",
            " death grave like sounds thee pleasure thou gods thou thought time worth thought\n",
            " place thee ne allwise doth art man morning power flight world pleasure returns\n",
            " place thee knows thought grieve thought deep thee grieve doth thought earth nature\n",
            " half thought pale thought thought man n d thought set nature cows big\n",
            " thought deep thee scarce thought deep thee air thought fear tis life live\n",
            " fear live live jewellery live idoll live grace years thee clear sold thou\n",
            " thought world bred time hands thou woe shall created ” hope ” pain\n",
            " ’ sudden tears pale youth act jewellery shall live think shall sky golden\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqJn2xZtWa7_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}