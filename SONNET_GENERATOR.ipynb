{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SONNET_GENERATOR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toukeer123/Adalovelace_day_Hackathon/blob/master/SONNET_GENERATOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7IOR_6yffU9"
      },
      "source": [
        "# **IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjrBqkQPD_xq"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import Regularizer\n",
        "import tensorflow.keras.utils as ku \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcIjR6RWXNr_",
        "outputId": "3a64aeef-2647-4eba-ad36-829967ee051a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuCBB-YzfrpG"
      },
      "source": [
        "# **READING, TOKENIZING AND CLEANING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "#Reading Data\n",
        "data = open(r'/content/drive/My Drive/dataset.txt').read()\n",
        "\n",
        "#Making sentences Lower case and spliting it \n",
        "sonnet_14 = data.lower().split(\"<eos>\")\n",
        "\n",
        "#Removing the un wanted terms from the data\n",
        "for i in range(len(sonnet_14)):\n",
        " sonnet_14[i] = sonnet_14[i].replace(\"\\n\",'')\n",
        " sonnet_14[i] = sonnet_14[i].replace(\"<eos>\",'')\n",
        " sonnet_14[i] = re.sub(\"[`~!@#$+%*:()'?-]\", ' ',sonnet_14[i]) \n",
        " sonnet_14[i] = remove_stopwords(sonnet_14[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTAqPjEwf88r"
      },
      "source": [
        "# **GENERATE VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlUrZhRiEJPT"
      },
      "source": [
        "#Generating the values for the data\n",
        "tokenizer.fit_on_texts(sonnet_14)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "input_sequences = []\n",
        "for line in sonnet_14:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUhBviehEOUi"
      },
      "source": [
        "max_sequence = max([len(x) for x in input_sequences])#selecting the maximum input sequence length\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence, padding='pre'))#padding prior zeros to the input sequence\n",
        "\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]#splitting the data into predictor and label\n",
        "                                                                                  \n",
        "label = ku.to_categorical(label, num_classes=total_words)#making the variables to be either 0 or 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hmLmG4LgJ5Q"
      },
      "source": [
        "# **CREATING A MODEL AND ITS LAYERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY9FX92qETno"
      },
      "source": [
        "model = Sequential() #Using sequential model\n",
        "model.add(Embedding(total_words, 50, input_length=max_sequence-1))  # Your Embedding Layer\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))  # An LSTM Layer\n",
        "model.add(Dropout(0.2))  #(# A dropout layer)\n",
        "model.add(LSTM(100))  #(# Another LSTM Layer)\n",
        "model.add(Dense(total_words/2, activation='relu'))  # A Dense Layer including regularizers\n",
        "model.add(Dense(total_words, activation='softmax'))  # A Dense Layer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Pick a loss function and an optimizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk8pa26MgXAE"
      },
      "source": [
        "# **TRAINING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIa4-HI0xjle",
        "outputId": "a6dfb7da-3fbf-4c3c-bb34-5619ead499ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train=model.fit(predictors,label,epochs=100,verbose=1)#traing the model on 100 epochs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8371/8371 [==============================] - 307s 37ms/step - loss: 7.1205 - accuracy: 0.0663\n",
            "Epoch 2/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 6.6212 - accuracy: 0.0824\n",
            "Epoch 3/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 6.2654 - accuracy: 0.0928\n",
            "Epoch 4/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 5.9854 - accuracy: 0.1012\n",
            "Epoch 5/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 5.7454 - accuracy: 0.1100\n",
            "Epoch 6/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 5.5269 - accuracy: 0.1182\n",
            "Epoch 7/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 5.3247 - accuracy: 0.1275\n",
            "Epoch 8/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 5.1280 - accuracy: 0.1374\n",
            "Epoch 9/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 4.9341 - accuracy: 0.1499\n",
            "Epoch 10/100\n",
            "8371/8371 [==============================] - 306s 36ms/step - loss: 4.7360 - accuracy: 0.1647\n",
            "Epoch 11/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 4.5243 - accuracy: 0.1820\n",
            "Epoch 12/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 4.3102 - accuracy: 0.2031\n",
            "Epoch 13/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 4.0884 - accuracy: 0.2260\n",
            "Epoch 14/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 3.8692 - accuracy: 0.2531\n",
            "Epoch 15/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 3.6648 - accuracy: 0.2798\n",
            "Epoch 16/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 3.4790 - accuracy: 0.3062\n",
            "Epoch 17/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 3.3138 - accuracy: 0.3301\n",
            "Epoch 18/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 3.1596 - accuracy: 0.3540\n",
            "Epoch 19/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 3.0245 - accuracy: 0.3756\n",
            "Epoch 20/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.8987 - accuracy: 0.3954\n",
            "Epoch 21/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.7814 - accuracy: 0.4160\n",
            "Epoch 22/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.6816 - accuracy: 0.4320\n",
            "Epoch 23/100\n",
            "8371/8371 [==============================] - 301s 36ms/step - loss: 2.5889 - accuracy: 0.4485\n",
            "Epoch 24/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.4967 - accuracy: 0.4641\n",
            "Epoch 25/100\n",
            "8371/8371 [==============================] - 302s 36ms/step - loss: 2.4175 - accuracy: 0.4780\n",
            "Epoch 26/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 2.3459 - accuracy: 0.4914\n",
            "Epoch 27/100\n",
            "8371/8371 [==============================] - 303s 36ms/step - loss: 2.2803 - accuracy: 0.5038\n",
            "Epoch 28/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 2.2188 - accuracy: 0.5147\n",
            "Epoch 29/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 2.1617 - accuracy: 0.5256\n",
            "Epoch 30/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 2.1092 - accuracy: 0.5357\n",
            "Epoch 31/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 2.0596 - accuracy: 0.5452\n",
            "Epoch 32/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 2.0154 - accuracy: 0.5536\n",
            "Epoch 33/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.9754 - accuracy: 0.5622\n",
            "Epoch 34/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.9346 - accuracy: 0.5707\n",
            "Epoch 35/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.9009 - accuracy: 0.5756\n",
            "Epoch 36/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.8650 - accuracy: 0.5845\n",
            "Epoch 37/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.8302 - accuracy: 0.5904\n",
            "Epoch 38/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.8004 - accuracy: 0.5969\n",
            "Epoch 39/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.7732 - accuracy: 0.6037\n",
            "Epoch 40/100\n",
            "8371/8371 [==============================] - 307s 37ms/step - loss: 1.7479 - accuracy: 0.6083\n",
            "Epoch 41/100\n",
            "8371/8371 [==============================] - 307s 37ms/step - loss: 1.7240 - accuracy: 0.6130\n",
            "Epoch 42/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.7029 - accuracy: 0.6173\n",
            "Epoch 43/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.6746 - accuracy: 0.6238\n",
            "Epoch 44/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.6555 - accuracy: 0.6277\n",
            "Epoch 45/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.6366 - accuracy: 0.6315\n",
            "Epoch 46/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.6164 - accuracy: 0.6366\n",
            "Epoch 47/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.5997 - accuracy: 0.6395\n",
            "Epoch 48/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.5831 - accuracy: 0.6434\n",
            "Epoch 49/100\n",
            "8371/8371 [==============================] - 306s 36ms/step - loss: 1.5658 - accuracy: 0.6469\n",
            "Epoch 50/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.5522 - accuracy: 0.6500\n",
            "Epoch 51/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.5333 - accuracy: 0.6540\n",
            "Epoch 52/100\n",
            "8371/8371 [==============================] - 307s 37ms/step - loss: 1.5208 - accuracy: 0.6569\n",
            "Epoch 53/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.5106 - accuracy: 0.6594\n",
            "Epoch 54/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4950 - accuracy: 0.6630\n",
            "Epoch 55/100\n",
            "8371/8371 [==============================] - 308s 37ms/step - loss: 1.4859 - accuracy: 0.6649\n",
            "Epoch 56/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4691 - accuracy: 0.6675\n",
            "Epoch 57/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.4607 - accuracy: 0.6710\n",
            "Epoch 58/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4479 - accuracy: 0.6735\n",
            "Epoch 59/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4383 - accuracy: 0.6757\n",
            "Epoch 60/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.4297 - accuracy: 0.6779\n",
            "Epoch 61/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.4206 - accuracy: 0.6800\n",
            "Epoch 62/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.4095 - accuracy: 0.6823\n",
            "Epoch 63/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.4052 - accuracy: 0.6832\n",
            "Epoch 64/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3933 - accuracy: 0.6857\n",
            "Epoch 65/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3863 - accuracy: 0.6877\n",
            "Epoch 66/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3759 - accuracy: 0.6903\n",
            "Epoch 67/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3721 - accuracy: 0.6907\n",
            "Epoch 68/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.3622 - accuracy: 0.6937\n",
            "Epoch 69/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3563 - accuracy: 0.6957\n",
            "Epoch 70/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3495 - accuracy: 0.6966\n",
            "Epoch 71/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3427 - accuracy: 0.6978\n",
            "Epoch 72/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3364 - accuracy: 0.7002\n",
            "Epoch 73/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3273 - accuracy: 0.7014\n",
            "Epoch 74/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3218 - accuracy: 0.7030\n",
            "Epoch 75/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3179 - accuracy: 0.7037\n",
            "Epoch 76/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3070 - accuracy: 0.7071\n",
            "Epoch 77/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.3061 - accuracy: 0.7067\n",
            "Epoch 78/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.3004 - accuracy: 0.7079\n",
            "Epoch 79/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.2958 - accuracy: 0.7088\n",
            "Epoch 80/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2888 - accuracy: 0.7110\n",
            "Epoch 81/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2838 - accuracy: 0.7121\n",
            "Epoch 82/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2808 - accuracy: 0.7130\n",
            "Epoch 83/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2792 - accuracy: 0.7135\n",
            "Epoch 84/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2739 - accuracy: 0.7149\n",
            "Epoch 85/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2645 - accuracy: 0.7175\n",
            "Epoch 86/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2645 - accuracy: 0.7171\n",
            "Epoch 87/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2578 - accuracy: 0.7185\n",
            "Epoch 88/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2558 - accuracy: 0.7196\n",
            "Epoch 89/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2489 - accuracy: 0.7212\n",
            "Epoch 90/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2473 - accuracy: 0.7217\n",
            "Epoch 91/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2408 - accuracy: 0.7233\n",
            "Epoch 92/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2365 - accuracy: 0.7241\n",
            "Epoch 93/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2329 - accuracy: 0.7244\n",
            "Epoch 94/100\n",
            "8371/8371 [==============================] - 306s 36ms/step - loss: 1.2295 - accuracy: 0.7256\n",
            "Epoch 95/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.2286 - accuracy: 0.7257\n",
            "Epoch 96/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.2253 - accuracy: 0.7273\n",
            "Epoch 97/100\n",
            "8371/8371 [==============================] - 306s 37ms/step - loss: 1.2226 - accuracy: 0.7273\n",
            "Epoch 98/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2192 - accuracy: 0.7288\n",
            "Epoch 99/100\n",
            "8371/8371 [==============================] - 305s 36ms/step - loss: 1.2174 - accuracy: 0.7289\n",
            "Epoch 100/100\n",
            "8371/8371 [==============================] - 304s 36ms/step - loss: 1.2141 - accuracy: 0.7305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pvb64LSgqpI"
      },
      "source": [
        "# **SAVING THE TRAINED MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpoO9NJIy3Qd",
        "outputId": "c7407248-85e0-483c-bffd-f1eadc895261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"jack1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rASNH2Fzg-Lk"
      },
      "source": [
        "# **LOADING TRAINED MODEL AND PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qbmrSROwEE9",
        "outputId": "216763a7-09a8-424d-bc49-7ca935174119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "#to read the .json and HDF5 file from the drive\n",
        "from tensorflow.keras.models import model_from_json\n",
        "json_file = open('/content/drive/My Drive/jack1.json','r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights('/content/drive/My Drive/model.h5')\n",
        "model = loaded_model\n",
        "#Feeding the trained model to predict the words\n",
        "def predict(seed_text , seed = 625):\n",
        "    for i in range( seed ):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=\n",
        "        max_sequence , padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0 )\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "        if i!=1 and i!=0:\n",
        "          if i%13==0:\n",
        "            seed_text+='\\n'\n",
        "    return seed_text\n",
        "\n",
        "\n",
        "#Giving input and printing the sonnet\n",
        "sonnet_14 = input()\n",
        "tk=predict(sonnet_14)\n",
        "nxt=tk.split(\"\\n\")[1:15]\n",
        "for i in range(len(d)):  \n",
        "  d[i] = re.sub(\"[,`~!@#$+%*:()'?-]\", '',d[i])\n",
        "gt=d.pop(0).split(\" \")\n",
        "if(gt[0].lower()=='d' or gt[0].lower()=='s'or gt[0].lower()=='è'):gt[0]=\"As\"\n",
        "for i in([\" \".join(gt).capitalize()]+nxt):\n",
        "  print(i)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wa wa kya bat\n",
            " beauty clear doth outcry hearts hand grace o soft thy mortal d form\n",
            " thy power day power man grave thy sounds time summer eyes thee true\n",
            " blame death grave like blue d art fair known tis art power country\n",
            " doth è spirit brown happy save thou strong gentle come air thee address\n",
            " lies doth fair cast dear death true billowy thy mother ‘ songs throng\n",
            " s door case arms thee grief dim nature cast eyes heard “ blue\n",
            " death grave like sounds thee pleasure thou gods thou thought time worth thought\n",
            " place thee ne allwise doth art man morning power flight world pleasure returns\n",
            " place thee knows thought grieve thought deep thee grieve doth thought earth nature\n",
            " half thought pale thought thought man n d thought set nature cows big\n",
            " thought deep thee scarce thought deep thee air thought fear tis life live\n",
            " fear live live jewellery live idoll live grace years thee clear sold thou\n",
            " thought world bred time hands thou woe shall created ” hope ” pain\n",
            " ’ sudden tears pale youth act jewellery shall live think shall sky golden\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqJn2xZtWa7_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}